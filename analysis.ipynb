{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Unpickling data:\n",
    "hr has reading every 5\n",
    "steps has reading per minute, or every 12 elements. ! Not true due to missing readings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "from collections import Counter\n",
    "from datetime import date\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n------------------------------------------------------\\nStarting cycle\\n------------------------------------------------------\")\n",
    "# Reference group average\n",
    "ref_avg_accuracy_test = run([0, 1, 7, 8, 9, 10], 45)\n",
    "print(\"Reference group done\")\n",
    "\n",
    "combinations = [\n",
    "[0, 1, 7, 8], \n",
    "[0, 1, 7, 9], [0, 1, 7, 10],\n",
    "[0, 1, 8, 9], [0, 1, 8, 10], [0, 1, 9, 10],\n",
    "[0, 7, 8, 9], [0, 7, 8, 10], [0, 7, 9, 10],\n",
    "[0, 8, 9, 10], [1, 7, 8, 9], [1, 7, 8, 10],\n",
    "[1, 7, 9, 10], [1, 8, 9, 10], [7, 8, 9, 10]\n",
    "]\n",
    "\n",
    "# Heart failure group average\n",
    "heart_failure_avg_accuracy_test = []\n",
    "\n",
    "# (Very) fit group average\n",
    "fit_avg_accuracy_test = []\n",
    "\n",
    "# Obese group average\n",
    "obese_avg_accuracy_test = []\n",
    "\n",
    "# Paroxysmal atrial fibrillation group average\n",
    "paroxysmal_atrial_fibrillation_avg_accuracy_test = []\n",
    "\n",
    "runs_per_combination = 3\n",
    "\n",
    "for run_index, combination in enumerate(combinations):\n",
    "    # Heart failure group\n",
    "    combination.extend([20, 24])\n",
    "    if not np.any(heart_failure_avg_accuracy_test):\n",
    "        heart_failure_avg_accuracy_test = run(combination, runs_per_combination)\n",
    "    else:\n",
    "        avg_accuracy_test = run(combination, runs_per_combination)\n",
    "        heart_failure_avg_accuracy_test = [(x + y) / 2 for x, y in zip(heart_failure_avg_accuracy_test, avg_accuracy_test)]\n",
    "    print(\"Run \" + str(run_index) + \"/\" + str(runs_per_combination*len(combinations)) + \" for 'heart failure' group done\")\n",
    "\n",
    "    combination.pop()  # Remove last element\n",
    "    combination.pop()  # Remove last element\n",
    "\n",
    "    # (Very) fit group\n",
    "    combination.extend([2, 15])\n",
    "    if not np.any(fit_avg_accuracy_test):\n",
    "        fit_avg_accuracy_test = run(combination, runs_per_combination)\n",
    "    else:\n",
    "        avg_accuracy_test = run(combination, runs_per_combination)\n",
    "        fit_avg_accuracy_test = [(x + y) / 2 for x, y in zip(fit_avg_accuracy_test, avg_accuracy_test)]\n",
    "    print(\"Run \" + str(run_index) + \"/\" + str(runs_per_combination*len(combinations)) + \" for '(very) fit' group done\")\n",
    "\n",
    "    combination.pop()  # Remove last element\n",
    "    combination.pop()  # Remove last element\n",
    "\n",
    "    # Obese group\n",
    "    combination.extend([3, 14])\n",
    "    if not np.any(obese_avg_accuracy_test):\n",
    "        obese_avg_accuracy_test = run(combination, runs_per_combination)\n",
    "    else:\n",
    "        avg_accuracy_test = run(combination, runs_per_combination)\n",
    "        obese_avg_accuracy_test = [(x + y) / 2 for x, y in zip(obese_avg_accuracy_test, avg_accuracy_test)]\n",
    "    print(\"Run \" + str(run_index) + \"/\" + str(runs_per_combination*len(combinations)) + \" for 'obese' group done\")\n",
    "\n",
    "    combination.pop()  # Remove last element\n",
    "    combination.pop()  # Remove last element\n",
    "\n",
    "    # Paroxysmal atrial fibrillation group\n",
    "    combination.extend([29, 25])\n",
    "    if not np.any(paroxysmal_atrial_fibrillation_avg_accuracy_test):\n",
    "        paroxysmal_atrial_fibrillation_avg_accuracy_test = run(combination, runs_per_combination)\n",
    "    else:\n",
    "        avg_accuracy_test = run(combination, runs_per_combination)\n",
    "        paroxysmal_atrial_fibrillation_avg_accuracy_test = [(x + y) / 2 for x, y in\n",
    "                                                           zip(paroxysmal_atrial_fibrillation_avg_accuracy_test,\n",
    "                                                               avg_accuracy_test)]\n",
    "    print(\"Run \" + str(run_index+1) + \"/\" + str(len(combinations)) + \" for 'paroxysmal atrial fibrillation' group done\")\n",
    "\n",
    "    combination.pop()  # Remove last element\n",
    "    combination.pop()  # Remove last element\n",
    "\n",
    "print(\"\\n------------------------------------------------------\\nFull cycle done!\\n------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ref_avg_accuracy_test, label='Reference group', color='blue')\n",
    "plt.plot(heart_failure_avg_accuracy_test, label='Heart failure group', color='green')\n",
    "plt.plot(fit_avg_accuracy_test, label='(Very) fit group', color='purple')\n",
    "plt.plot(obese_avg_accuracy_test, label='Obese group', color='red')\n",
    "plt.plot(paroxysmal_atrial_fibrillation_avg_accuracy_test, label='Paroxysmal atrial fibrillation group', color='orange')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks(np.arange(0, 1, 0.1))\n",
    "plt.legend(loc='lower right', fontsize=11)  # Move the legend to the bottom right\n",
    "\n",
    "# Add horizontal raster lines\n",
    "raster_values = np.arange(0, 1, 0.1)\n",
    "for value in raster_values:\n",
    "    plt.axhline(y=value, color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables and their corresponding file names\n",
    "variables = [\n",
    "    ('reference_hr_data', ['xxxxxx.pkl', 'xxxxxx.pkl',\n",
    "                           'xxxxxx.pkl', 'xxxxxx.pkl',\n",
    "                           'xxxxxx.pkl', 'xxxxxx.pkl']),\n",
    "    ('fit_hr_data', ['xxxxxx.pkl', 'xxxxxx.pkl']),\n",
    "    ('obese_hr_data', ['xxxxxx.pkl', 'xxxxxx.pkl']),\n",
    "    ('heart_failure_hr_data', ['xxxxxx.pkl', 'xxxxxx.pkl']),\n",
    "    ('paroxysmal_hr_data', ['xxxxxx.pkl', 'xxxxxx.pkl'])\n",
    "]\n",
    "\n",
    "# Create the figure and axis with smaller figsize\n",
    "fig, ax = plt.subplots(figsize=(5.7, 4))\n",
    "#fig.suptitle('Hourly Average Heart Rate')\n",
    "\n",
    "# Define the colors for each group\n",
    "colors = ['blue', 'purple', 'red', 'green', 'orange']\n",
    "\n",
    "# Define the labels for each group\n",
    "labels = ['Reference group', 'Fit group', 'Obese group', 'Heart failure group', 'Paroxysmal atrial fibrillation group']\n",
    "\n",
    "# Iterate over the variables and plot the hourly averages\n",
    "for (var_name, file_names), color, label in zip(variables, colors, labels):\n",
    "    # Load and combine the variables from the files\n",
    "    combined_variable = pd.DataFrame()\n",
    "    for file_name in file_names:\n",
    "        with open(file_name, 'rb') as f:\n",
    "            variable = pkl.load(f)\n",
    "        combined_variable = pd.concat([combined_variable, variable], ignore_index=True)\n",
    "\n",
    "    # Convert the time column to a pandas DateTimeIndex\n",
    "    combined_variable['time'] = pd.to_datetime(combined_variable['time'])\n",
    "\n",
    "    # Calculate the hourly averages\n",
    "    hourly_averages = combined_variable.groupby(combined_variable['time'].dt.hour)['hr'].mean()\n",
    "\n",
    "    # Plot the hourly averages with labels\n",
    "    ax.plot(hourly_averages.index, hourly_averages.values, label=label, color=color, linewidth=1)\n",
    "\n",
    "# Set the x-axis label\n",
    "ax.set_xlabel('Hour of the day')\n",
    "\n",
    "# Set the y-axis label\n",
    "ax.set_ylabel('Heart rate')\n",
    "\n",
    "# Set the y-axis limits\n",
    "ax.set_ylim(20, 90)\n",
    "\n",
    "# Add horizontal raster lines\n",
    "raster_values = np.arange(30, 90, 10)\n",
    "for value in raster_values:\n",
    "    plt.axhline(y=value, color='gray', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a legend\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run(keep, runs):\n",
    "        \n",
    "    # For reading in the pickled files\n",
    "\n",
    "    subjects_dir = '/subjects/'\n",
    "    subjects = os.listdir(subjects_dir)\n",
    "    subjects_data = []\n",
    "    meta_data = []\n",
    "    s = []\n",
    "    file_names = [\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\",\n",
    "        \"xxxxxx.pkl\"\n",
    "    ]\n",
    "\n",
    "    # The following subjects have insufficient amount of data, so ignore them\n",
    "    leave_out = [2, 8, 11, 12, 13, 16, 24, 27, 29, 30, 31, 32, 33, 34, 35, 39, 40, 42, 43, 46, 47, 48, 49]\n",
    "\n",
    "    file_names = [file_names[i] for i in range(len(file_names)) if i not in leave_out]\n",
    "\n",
    "    # This is to filter out to get specific cardiac diagnosis\n",
    "    file_names = [file_names[i] for i in keep]\n",
    "\n",
    "    file_map = {index: file_name for index, file_name in enumerate(file_names)}\n",
    "    # Iterate over file_names and load subject data for valid indices\n",
    "    for index, file_name in file_map.items():\n",
    "        #if index not in leave_out:\n",
    "        subject_file = os.path.join(subjects_dir, file_name)\n",
    "        # Load subject data from file\n",
    "        with open(subject_file, 'rb') as f:\n",
    "            subject_data = pkl.load(f)\n",
    "            s.append(file_name)\n",
    "            subjects_data.append(subject_data)\n",
    "        with open(\"meta.pkl\", \"rb\") as f:\n",
    "            meta_object = pkl.load(f)\n",
    "            meta_data.append(meta_object)\n",
    "\n",
    "    subjects = len(subjects_data)\n",
    "\n",
    "    dx_cardiac_diagnosis_list = meta_object['dx_cardiac_diagnosis'].tolist()\n",
    "\n",
    "    # Remove the last digit from each entry in data_platform_id\n",
    "    data_platform_id_cleaned = [platform_id[:-1] if isinstance(platform_id, str) else platform_id for platform_id in meta_object['data_platform_id']]\n",
    "    filtered_data_platform_id = [platform_id if platform_id in [name[:-4] for name in file_names] else None for platform_id in data_platform_id_cleaned]\n",
    "\n",
    "    filtered_data_platform_id_length = len(filtered_data_platform_id)\n",
    "    non_none_entries_count = len([entry for entry in filtered_data_platform_id if entry is not None])\n",
    "\n",
    "    filtered_participant_id = [entry if str(entry) + \".pkl\" in file_names else None for entry in meta_object['participant_id']]\n",
    "\n",
    "    filtered_participant_id_length = len(filtered_participant_id)\n",
    "    filtered_participant_id_count = sum(entry is not None for entry in filtered_participant_id)\n",
    "\n",
    "    combined_ids_meta = [dp_id if dp_id is not None else p_id for dp_id, p_id in zip(filtered_data_platform_id, filtered_participant_id)]\n",
    "\n",
    "    combined_ids_count = sum(1 for entry in combined_ids_meta if entry is not None)\n",
    "\n",
    "    meta_indices = [combined_ids_meta.index(name[:-4]) if name[:-4] in combined_ids_meta else None for name in file_names]\n",
    "    cardiac_status = []\n",
    "    indices = []\n",
    "\n",
    "    for index in meta_indices:\n",
    "        if index is not None:\n",
    "            cardiac_status.append(dx_cardiac_diagnosis_list[index])\n",
    "            indices.append(index)\n",
    "        else:\n",
    "            cardiac_status.append(None)\n",
    "            indices.append(None)\n",
    "\n",
    "    frequency = Counter(cardiac_status)\n",
    "\n",
    "    for entry, count in frequency.items():\n",
    "        entry_indices = [i for i, status in enumerate(cardiac_status) if status == entry]\n",
    "\n",
    "    minute = 12\n",
    "    hour = minute * 60\n",
    "    day = hour * 24 #17280\n",
    "\n",
    "    def sliding_window (xs, window_size, stride):\n",
    "        ps = []\n",
    "        if len(xs) <= window_size:\n",
    "            return []\n",
    "        for i in range(len(xs) - window_size + 1):\n",
    "            if i % stride == 0:\n",
    "                ps.append(xs[i: i + window_size])\n",
    "        return ps\n",
    "\n",
    "    def normalize_vars(arr):\n",
    "        valid_values = arr[~np.isnan(arr)]\n",
    "        mean = np.mean(valid_values)\n",
    "        sd = np.std(valid_values)\n",
    "        return(mean, sd)\n",
    "\n",
    "    def normalize(arr, mean, sd):\n",
    "        templist = []\n",
    "        for i in range(len(arr)):\n",
    "            templist.append((arr[i] - mean) / sd)\n",
    "        return templist\n",
    "\n",
    "    test1 = np.array([1, 2, 3 ,4, 5, 6])\n",
    "    vrs1 = normalize_vars(test1)\n",
    "    test2 = np.array([np.NaN, np.NaN, np.NaN, 1, np.NaN, np.NaN])\n",
    "    vrs2 = normalize_vars(test2)\n",
    "\n",
    "    # Split data into days\n",
    "    subjects = len(subjects_data) \n",
    "    subjects_data_by_day = []\n",
    "    for i in range(subjects): # splits dataframe for each user into days\n",
    "        df = subjects_data[i]\n",
    "        day = df['time'].dt.floor(\"D\")\n",
    "        agg = df.groupby([day])\n",
    "        smallList = []\n",
    "        for i in agg:\n",
    "            # Take only days of which there are enough readings! \n",
    "            if (len(i[1]) >= 14000): \n",
    "                steps = i[1].steps\n",
    "                newsteps = steps[~np.isnan(steps)]\n",
    "                smallList.append(i[1].head(14000)) \n",
    "        subjects_data_by_day.append(smallList)\n",
    "\n",
    "    window_size = day \n",
    "    stride = window_size \n",
    "    balance = 50 \n",
    "\n",
    "    identity = np.identity(subjects) # used for one hot encoding\n",
    "\n",
    "    # Convert dataframe to np heart rate\n",
    "    subjects_data_hr = []\n",
    "    for i in range(subjects):\n",
    "        smallList = []\n",
    "        for j in range(len(subjects_data_by_day[i][:balance])):\n",
    "            smallList.append(subjects_data_by_day[i][j].hr)\n",
    "        subjects_data_hr.append(smallList)\n",
    "\n",
    "    # Convert to train test\n",
    "    X_train_users_temp = []\n",
    "    X_test_users_temp = []\n",
    "    Y_train_users = []\n",
    "    Y_test_users = []\n",
    "\n",
    "    for i in range(subjects):\n",
    "        X_temp = np.array(subjects_data_hr[i])\n",
    "        Y_temp = np.full(shape=len(X_temp), fill_value=i)\n",
    "        X_train_temp, X_test_temp, Y_train_temp, Y_test_temp = train_test_split(X_temp, Y_temp, test_size=0.2, shuffle=False)\n",
    "\n",
    "        X_train_users_temp.append(X_train_temp)\n",
    "        X_test_users_temp.append(X_test_temp)\n",
    "\n",
    "\n",
    "    # Normalize on variables from train set\n",
    "    mean, sd = normalize_vars(np.concatenate(np.concatenate(X_train_users_temp)))\n",
    "    X_train_users_hr = []\n",
    "    X_test_users_hr = []\n",
    "    for i in range(subjects):\n",
    "        X_train_temp = np.array(normalize(X_train_users_temp[i], mean, sd))\n",
    "        X_test_temp = np.array(normalize(X_test_users_temp[i], mean, sd))\n",
    "\n",
    "        X_train_users_hr.append(X_train_temp)\n",
    "        X_test_users_hr.append(X_test_temp)\n",
    "\n",
    "    # Convert dataframe to np steps\n",
    "    subjects_data_steps = []\n",
    "    for i in range(subjects):\n",
    "        smallList = []\n",
    "        for j in range(len(subjects_data_by_day[i][:balance])):\n",
    "            smallList.append(subjects_data_by_day[i][j].steps)\n",
    "        subjects_data_steps.append(smallList)\n",
    "\n",
    "    # Convert to train test\n",
    "    X_train_users_temp = []\n",
    "    X_test_users_temp = []\n",
    "    Y_train_users = []\n",
    "    Y_test_users = []\n",
    "    for i in range(subjects):\n",
    "        X_temp = np.array(subjects_data_steps[i])\n",
    "        Y_temp = np.full(shape=len(X_temp), fill_value=i)\n",
    "        X_train_temp, X_test_temp, Y_train_temp, Y_test_temp = train_test_split(X_temp, Y_temp, test_size=0.2, shuffle=False)\n",
    "\n",
    "        X_train_users_temp.append(X_train_temp)\n",
    "        X_test_users_temp.append(X_test_temp)\n",
    "\n",
    "    # Normalize on variables from train set\n",
    "    mean, sd = normalize_vars(np.concatenate(np.concatenate(X_train_users_temp)))\n",
    "\n",
    "    X_train_users_steps_temp = []\n",
    "    X_test_users_steps_temp = []\n",
    "    for i in range(subjects):\n",
    "        X_train_temp = np.array(normalize(X_train_users_temp[i], mean, sd))\n",
    "        X_test_temp = np.array(normalize(X_test_users_temp[i], mean, sd))\n",
    "\n",
    "        X_train_users_steps_temp.append(X_train_temp)\n",
    "        X_test_users_steps_temp.append(X_test_temp)\n",
    "\n",
    "    # Convert nan to unrealistic value\n",
    "    X_train_users_steps = []\n",
    "    X_test_users_steps = []\n",
    "    for i in range(subjects):\n",
    "        tempList = []\n",
    "        for j in range(len(X_train_users_steps_temp[i])):\n",
    "            window = X_train_users_steps_temp[i][j]\n",
    "            window = np.where(np.isnan(window), mean, window)\n",
    "            tempList.append(window)\n",
    "        X_train_users_steps.append(tempList)\n",
    "    for i in range(subjects):\n",
    "        tempList = []\n",
    "        for j in range(len(X_test_users_steps_temp[i])):\n",
    "            window = X_test_users_steps_temp[i][j]\n",
    "            window = np.where(np.isnan(window), mean, window)\n",
    "            tempList.append(window)\n",
    "        X_test_users_steps.append(tempList)\n",
    "\n",
    "    # Apply sliding window\n",
    "    X_train_hr = []  # List to store training windows\n",
    "    X_train_steps = []  # List to store training windows\n",
    "    X_test_hr = []  # List to store testing windows\n",
    "    X_test_steps = []  # List to store testing windows\n",
    "    Y_train_hr = []  # List to store training labels\n",
    "    Y_train_steps = []  # List to store training labels\n",
    "    Y_test_hr = []  # List to store testing labels\n",
    "    Y_test_steps = []  # List to store testing labels\n",
    "    train_len = int(balance * 0.8) #len(sliding_window(X_train_users_hr[10], window_size, stride)) # hardcoded :sad\n",
    "    test_len = int(balance * 0.2) # len(sliding_window(X_test_users_hr[10], window_size, stride)) # also hardcoded :sad\n",
    "    for j in range(train_len):\n",
    "        for i in range(subjects):\n",
    "            windows_train_hr = X_train_users_hr[i][j]\n",
    "            windows_train_steps = X_train_users_steps[i][j]\n",
    "            X_train_hr.append(np.array([windows_train_hr]))\n",
    "            X_train_steps.append(np.array([windows_train_steps]))\n",
    "            Y_train_hr.append(identity[i])\n",
    "            Y_train_steps.append(identity[i])\n",
    "\n",
    "    for j in range(test_len):\n",
    "        for i in range(subjects):\n",
    "            windows_test_hr = X_test_users_hr[i][j]\n",
    "            windows_test_steps = X_test_users_steps[i][j]\n",
    "            X_test_hr.append(np.array([windows_test_hr]))\n",
    "            X_test_steps.append(np.array([windows_test_steps]))\n",
    "            Y_test_hr.append(identity[i])\n",
    "            Y_test_steps.append(identity[i])\n",
    "\n",
    "    X_train_hr, X_val_hr, Y_train_hr, Y_val_hr = train_test_split(X_train_hr, Y_train_hr, test_size=0.2, random_state=42)\n",
    "    X_train_steps, X_val_steps, Y_train_steps, Y_val_steps = train_test_split(X_train_steps, Y_train_steps, test_size=0.2, random_state=42)\n",
    "\n",
    "    class CNN_LSTM(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, kernel_size, stride, subjects):\n",
    "            super(CNN_LSTM, self).__init__()\n",
    "\n",
    "            self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride)\n",
    "            self.relu1 = nn.ReLU()\n",
    "            self.dropout1 = nn.Dropout(0.1)\n",
    "            self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "            self.conv2 = nn.Conv1d(out_channels, 8, kernel_size=128, stride=32)\n",
    "            self.relu2 = nn.ReLU()\n",
    "            self.dropout2 = nn.Dropout(0.1)\n",
    "            self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.lstm = nn.LSTM(input_size=856, hidden_size=128, num_layers=1, batch_first=True)\n",
    "            self.linear = nn.Linear(128, subjects)\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = self.relu1(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = self.maxpool1(x)\n",
    "\n",
    "            x = self.conv2(x)\n",
    "            x = self.relu2(x)\n",
    "            x = self.dropout2(x)\n",
    "            x = self.maxpool2(x)\n",
    "\n",
    "            x = self.flatten(x)\n",
    "            x = x.view(x.size(0), 1, -1)  # Reshape the tensor to have a sequence dimension\n",
    "\n",
    "            x, _ = self.lstm(x)\n",
    "            x = x[:, -1, :]  # Select the last LSTM output of the sequence\n",
    "            x = self.linear(x)\n",
    "            x = self.softmax(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Set the number of runs\n",
    "    num_runs = runs\n",
    "\n",
    "    # Create empty lists to store the results of each run\n",
    "    accuracy_train_all = []\n",
    "    accuracy_val_all = []\n",
    "    accuracy_test_all = []\n",
    "    losses_all = []\n",
    "    losses_val_all = []\n",
    "    losses_test_all = []\n",
    "    final_class_train_all = []\n",
    "    final_class_test_all = []\n",
    "\n",
    "    # Perform multiple runs\n",
    "    for run in range(num_runs):\n",
    "\n",
    "        # Hyperparameters\n",
    "        input_size = 14000  \n",
    "        lr = 0.001\n",
    "        epochs = 50\n",
    "        batch_size = 256\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Define the model architecture\n",
    "        model = CNN_LSTM(in_channels=2, out_channels=4, kernel_size=5, stride=1, subjects=subjects).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        accuracy_test = []\n",
    "        losses = []\n",
    "        accuracy_train = []\n",
    "        accuracy_val = []\n",
    "\n",
    "\n",
    "        accuracy = []\n",
    "        losses = []\n",
    "        losses_test = []\n",
    "        losses_val = []\n",
    "        accuracy_train = []\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            correct_class_train = np.zeros(subjects)\n",
    "            total_class_train = np.zeros(subjects)\n",
    "            count_train = 0\n",
    "\n",
    "            # Training\n",
    "            model.train()\n",
    "\n",
    "            class_correct_train = [0] * subjects\n",
    "            class_total_train = [0] * subjects\n",
    "            for i in range(0, len(X_train_hr), batch_size):\n",
    "                remaining_samples = min(batch_size, len(X_train_hr) - i)\n",
    "                X_batch_hr = torch.tensor(np.array(X_train_hr[i: i + remaining_samples]), dtype=torch.float32)\n",
    "                X_batch_steps = torch.tensor(np.array(X_train_steps[i: i + remaining_samples]), dtype=torch.float32)\n",
    "                Y_batch = torch.tensor(np.array(Y_train_hr[i: i + remaining_samples]))\n",
    "                y_pred = model(torch.cat((X_batch_hr.to(device), X_batch_steps.to(device)), dim=1))\n",
    "\n",
    "                loss = loss_fn(y_pred, Y_batch.to(device))\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_correct_train = np.argmax(Y_batch.detach().cpu().numpy(), axis=1)\n",
    "                correct_class_train += np.sum(np.argmax(y_pred.detach().cpu().numpy(), axis=1) == y_correct_train)\n",
    "\n",
    "                count_train += np.sum(np.argmax(y_pred.detach().cpu().numpy(), axis=1) == y_correct_train)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Compute accuracy per class\n",
    "                y_pred_labels = np.argmax(y_pred.detach().cpu().numpy(), axis=1)\n",
    "                for j in range(len(y_correct_train)):\n",
    "                    class_total_train[y_correct_train[j]] += 1\n",
    "                    if y_correct_train[j] == y_pred_labels[j]:\n",
    "                        class_correct_train[y_correct_train[j]] += 1\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            accuracy_train.append(count_train / len(X_train_hr))\n",
    "\n",
    "            # Compute accuracy per class\n",
    "            final_class_train = [class_correct_train[i] / class_total_train[i] if class_total_train[i] > 0 else 0 for i in range(subjects)]\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                correct_class_val = np.zeros(subjects)\n",
    "                total_class_val = np.zeros(subjects)\n",
    "                count_val = 0\n",
    "                total_loss_val = 0\n",
    "\n",
    "                for j in range(len(X_val_hr)):\n",
    "                    X_batch_hr = np.reshape(X_val_hr[j], (1, 1, input_size))\n",
    "                    X_batch_steps = np.reshape(X_val_steps[j], (1, 1, input_size))\n",
    "                    tensor_hr = torch.tensor(X_batch_hr, dtype=torch.float32).to(device)\n",
    "                    tensor_steps = torch.tensor(X_batch_steps, dtype=torch.float32).to(device)\n",
    "                    y_pred = model(torch.cat((tensor_hr, tensor_steps), dim=1))\n",
    "                    #y_pred = model(tensor_hr)\n",
    "                    y_correct = np.argmax(Y_val_hr[j])\n",
    "                    total_class_val[y_correct] += 1\n",
    "\n",
    "                    if np.argmax(y_pred.detach().cpu().numpy()) == y_correct:\n",
    "                        correct_class_val[y_correct] += 1\n",
    "                        count_val += 1\n",
    "\n",
    "                    # Compute loss for validation\n",
    "                    Y_batch_val = torch.tensor(np.array([Y_val_hr[j]])).to(device)  # Wrap target in a list to match batch size\n",
    "                    loss_val = loss_fn(y_pred, Y_batch_val)\n",
    "                    total_loss_val += loss_val.item()\n",
    "\n",
    "                losses_val.append(total_loss_val / len(X_val_hr))\n",
    "                accuracy_val.append(count_val / len(X_val_hr))\n",
    "\n",
    "            # Testing\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                correct_class_test = np.zeros(subjects)\n",
    "                total_class_test = np.zeros(subjects)\n",
    "                count_test = 0\n",
    "                total_loss_test = 0\n",
    "                #print(len(X_test_hr)*2/6)\n",
    "                for k in range(int(len(X_test_hr)*2/6)):\n",
    "                    X_batch_hr = np.reshape(X_test_hr[k], (1, 1, input_size))\n",
    "                    X_batch_steps = np.reshape(X_test_steps[k], (1, 1, input_size))\n",
    "                    tensor_hr = torch.tensor(X_batch_hr, dtype=torch.float32).to(device)\n",
    "                    tensor_steps = torch.tensor(X_batch_steps, dtype=torch.float32).to(device)\n",
    "                    #y_pred = model(tensor_hr)\n",
    "                    y_pred = model(torch.cat((tensor_hr, tensor_steps), dim=1))\n",
    "                    #print(np.shape(tensor_hr))\n",
    "                    y_correct = np.argmax(Y_test_hr[k])\n",
    "                    total_class_test[y_correct] += 1\n",
    "\n",
    "                    if np.argmax(y_pred.detach().cpu().numpy()) == y_correct:\n",
    "                        correct_class_test[y_correct] += 1\n",
    "                        count_test += 1\n",
    "\n",
    "                    # Compute loss for testing\n",
    "                    Y_batch_test = torch.tensor(np.array([Y_test_hr[k]])).to(device)  # Wrap target in a list to match batch size  \n",
    "                    loss_test = loss_fn(y_pred, Y_batch_test)\n",
    "                    total_loss_test += loss_test.item()\n",
    "\n",
    "                losses_test.append(total_loss_test / int(len(X_test_hr)*2/6))\n",
    "                accuracy_test.append(count_test / int(len(X_test_hr)*2/6))\n",
    "                #print(model)\n",
    "\n",
    "            final_class_test = correct_class_test / total_class_test\n",
    "\n",
    "            #print(f\"Run {run + 1}/{num_runs}\")\n",
    "            #print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "            #print(f\"  Train Loss: {losses[-1]:.4f} - Train Accuracy: {accuracy_train[-1]*100:.2f}%\")\n",
    "            #print(f\"  Validation Accuracy: {accuracy_val[-1]*100:.2f}%\")\n",
    "            #print(f\"  Test Accuracy: {accuracy_test[-1]*100:.2f}%\")\n",
    "            #print()\n",
    "\n",
    "            # Store the results of the current run\n",
    "            accuracy_train_all.append(accuracy_train)\n",
    "            accuracy_val_all.append(accuracy_val)\n",
    "            accuracy_test_all.append(accuracy_test)\n",
    "            losses_all.append(losses)\n",
    "            losses_val_all.append(losses_val)\n",
    "            losses_test_all.append(losses_test)\n",
    "            final_class_train_all.append(final_class_train)\n",
    "            final_class_test_all.append(final_class_test)\n",
    "\n",
    "    # Convert the lists to NumPy arrays for easier averaging\n",
    "    accuracy_train_all = np.array(accuracy_train_all)\n",
    "    accuracy_val_all = np.array(accuracy_val_all)\n",
    "    accuracy_test_all = np.array(accuracy_test_all)\n",
    "    losses_all = np.array(losses_all)\n",
    "    losses_val_all = np.array(losses_val_all)\n",
    "    losses_test_all = np.array(losses_test_all)\n",
    "    final_class_train_all = np.array(final_class_train_all)\n",
    "    final_class_test_all = np.array(final_class_test_all)\n",
    "\n",
    "    # Calculate the average results across runs\n",
    "    avg_accuracy_train = np.mean(accuracy_train_all, axis=0)\n",
    "    avg_accuracy_val = np.mean(accuracy_val_all, axis=0)\n",
    "    avg_accuracy_test = np.mean(accuracy_test_all, axis=0)\n",
    "    avg_losses = np.mean(losses_all, axis=0)\n",
    "    avg_losses_val = np.mean(losses_val_all, axis=0)\n",
    "    avg_losses_test = np.mean(losses_test_all, axis=0)\n",
    "\n",
    "    # Calculate average testing accuracy across all runs\n",
    "    avg_test_accuracy = np.mean(avg_accuracy_test) * 100\n",
    "    \n",
    "    return avg_accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
